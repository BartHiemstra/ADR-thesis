{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nyYzxWzBJjX7"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries.\n",
        "import io\n",
        "import json\n",
        "import requests\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Scrape metadata about documents from TweedeKamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GlQReN6xPiDP"
      },
      "outputs": [],
      "source": [
        "# Define root url of API.\n",
        "api_root_url = 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/Document'\n",
        "\n",
        "# Define an empty Dataframe for the retrieved documents.\n",
        "df_documents = pd.DataFrame()\n",
        "\n",
        "# Define a separate empty Dataframe for the retrieved document texts.\n",
        "df_documents_text = pd.DataFrame(columns = ['id', 'text'])\n",
        "\n",
        "# Define the amount of batches (=250 documents) to scrape from the API.\n",
        "# E.g. 10 batches = the first 2500 documents.\n",
        "\n",
        "num_batches = 2500 #This will attempt to scrape all documents (n = 2500*250), but will likely be restricted by API security measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L16MWFD9JpI5"
      },
      "outputs": [],
      "source": [
        "# Retrieve Documents from API, 250 at a time (API restriction).\n",
        "\n",
        "for i in range(0, num_batches):\n",
        "  # Define the query for retrieving the current batch of documents.\n",
        "  api_query_batch = '?$skip=' + str(i * 250)\n",
        "  \n",
        "  # Send request and store response.\n",
        "  response = requests.get(api_root_url + api_query_batch)\n",
        "  response = response.json()\n",
        "\n",
        "  # Convert JSON response to Dataframe\n",
        "  df_batch = pd.json_normalize(response, 'value')\n",
        "\n",
        "  # Append the new batch to the existing collection.\n",
        "  df_documents = pd.concat([df_documents, df_batch], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert dates to a Pandas date format.\n",
        "df_documents['date'] = pd.to_datetime(df_documents['Datum'], dayfirst=True)\n",
        "\n",
        "# Then sort on date.\n",
        "df_documents = df_documents.sort_values(by='date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save the scraped data.\n",
        "df_documents.to_csv('api_documents.csv')\n",
        "df_documents.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Download and extract textual data associated with each scraped metadata-row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the content detection+extraction framework Apache Tika, ported to Python.\n",
        "from tika import parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare new DataFrame to hold extracted textual data.\n",
        "df_documents_text = pd.DataFrame(columns = ['id', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert any textual files attached to documents in df_documents to text and add to the df_documents_text Dataframe.\n",
        "\n",
        "for index, row in df_documents.iterrows():\n",
        "    \n",
        "  # If document does not contain attached pdf file, don't try extracting text.\n",
        "  if pd.isnull(row['ContentType']):\n",
        "    df_documents_text = pd.concat([df_documents_text, pd.DataFrame.from_records([{'id' : row['Id'], 'date' : row['Datum'], 'text' : ''}])])\n",
        "    continue\n",
        "    \n",
        "  # Build query based on document ID.\n",
        "  query = '(' + row['Id'] + ')/resource'\n",
        "\n",
        "  # Get attached text file from the API.\n",
        "  response = requests.get(api_root_url + query)\n",
        "\n",
        "  # Parse the text file using apache tika.\n",
        "  parsed_data = parser.from_buffer(response.content)\n",
        "  document_text = parsed_data['content']\n",
        "\n",
        "  # Append the contents to the df_contents_text dataframe.\n",
        "  df_documents_text = pd.concat([df_documents_text, pd.DataFrame.from_records([{'id' : row['Id'], 'date' : row['Datum'], 'text' : document_text}])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save as csv file.\n",
        "df_documents_text.to_csv('kamers_text.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
